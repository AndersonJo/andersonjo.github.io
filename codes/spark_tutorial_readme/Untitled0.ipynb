{
 "metadata": {
  "name": "",
  "signature": "sha256:5ba45cc2810f8a46d433c88406c16221d951214f58fe22d91d79e5d1eb5dbadf"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark import SparkConf, SparkContext\n",
      "\n",
      "spconf = SparkConf().setMaster('local').setAppName('Tutorial')\n",
      "sc = SparkContext(conf=spconf)\n",
      "\n",
      "textFile = sc.textFile('README.md')\n",
      "print textFile.count() # 36 the number of lines\n",
      "# print textFile.first()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Tutorial, master=local) created by __init__ at <ipython-input-1-4071f3e0fa18>:4 ",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-2-21170fc6e2e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mspconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'local'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tutorial'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtextFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'README.md'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/anderson/apps/spark-1.6.0-bin-hadoop2.6/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
        "\u001b[0;32m/home/anderson/apps/spark-1.6.0-bin-hadoop2.6/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway)\u001b[0m\n\u001b[1;32m    259\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 261\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    262\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Tutorial, master=local) created by __init__ at <ipython-input-1-4071f3e0fa18>:4 "
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "textFile = sc.textFile('README.md')\n",
      "\n",
      "def combine(a, b):\n",
      "    print 'Combine:', a, b\n",
      "    return a + b\n",
      "\n",
      "wordCounts = textFile.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(combine)\n",
      "print sorted(wordCounts.collect(), key=lambda d: d[1], reverse=True)\n",
      "\n",
      "\n",
      "print wordCounts.collect()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u'the', 10), (u'*', 6), (u'Spark', 5), (u'for', 4), (u'of', 4), (u'with', 4), (u'examples', 4), (u'run', 3), (u'and', 3), (u'have', 3), (u'a', 3), (u'Learning', 2), (u'assembly', 2), (u'dependencies', 2), (u'package', 2), (u'or', 2), (u'These', 2), (u'===', 2), (u'versions', 2), (u'require', 2), (u'can', 2), (u'install', 2), (u'1.3', 2), (u'in', 2), (u'Python', 2), (u'-', 2), (u'also', 2), (u'build', 2), (u'Examples', 2), (u'===============', 1), (u'all', 1), (u'code', 1), (u'Requirements', 1), (u'just', 1), (u'CRAN', 1), (u'We', 1), (u'jar', 1), (u'to', 1), (u'$SPARK_HOME;', 1), (u'directory.', 1), (u'On', 1), (u'updated', 1), (u'./sbt/sbt', 1), (u'==', 1), (u'they', 1), (u'ChapterSixExample', 1), (u'apt-get', 1), (u'compiler', 1), (u'./bin/pyspark', 1), (u'either', 1), (u'small', 1), (u'--class', 1), (u'From', 1), (u'JDK', 1), (u'Scala', 1), (u'scala-lang.org', 1), (u'are', 1), (u'scala', 1), (u'libraries', 1), (u'Spark\".', 1), (u'minimal', 1), (u'./bin/spark-submit', 1), (u'book.', 1), (u'mini-complete-example', 1), (u'job', 1), (u'alone', 1), (u'spark', 1), (u'&', 1), (u'against', 1), (u'OR', 1), (u'stand', 1), (u'slightly', 1), (u'mvn', 1), (u'number', 1), (u'cd', 1), (u'../learning-spark-examples/target/scala-2.10/learning-spark-examples-assembly-0.0.1.jar', 1), (u'./src/python/[example]', 1), (u'\"Learning', 1), (u'your', 1), (u'long', 1), (u'java', 1), (u'Protobuf', 1), (u'files.', 1), (u'create', 1), (u'been', 1), (u'[![Learning', 1), (u'R', 1), (u'be', 1), (u'copy', 1), (u'than', 1), (u'example', 1), (u'1.7', 1), (u'file', 1), (u'sudo', 1), (u'Submit', 1), (u'an', 1), (u'spark-submit', 1), (u'as', 1), (u'You', 1), (u'com.oreilly.learningsparkexamples.[lang].[example]', 1), (u'different', 1), (u'you', 1), (u'Imap', 1), (u'higher', 1), (u'added', 1), (u'may', 1), (u'urllib3', 1), (u'Spark](http://akamaicovers.oreilly.com/images/0636920028512/cat.gif)](http://www.jdoqocy.com/click-7645222-11260198?url=http%3A%2F%2Fshop.oreilly.com%2Fproduct%2F0636920028512.do%3Fcmp%3Daf-strata-books-videos-product_cj_9781449358600_%2525zp&cjsku=0636920028512)', 1), (u'running', 1), (u'such', 1), (u'The', 1), (u'debian', 1), (u'2.10.3', 1), (u'required', 1), (u'script', 1), (u'so', 1), (u'protobuf-compiler', 1)]\n",
        "[(u'===============', 1), (u'all', 1), (u'code', 1), (u'Requirements', 1), (u'just', 1), (u'CRAN', 1), (u'We', 1), (u'jar', 1), (u'to', 1), (u'$SPARK_HOME;', 1), (u'directory.', 1), (u'On', 1), (u'updated', 1), (u'./sbt/sbt', 1), (u'==', 1), (u'they', 1), (u'ChapterSixExample', 1), (u'apt-get', 1), (u'Spark', 5), (u'compiler', 1), (u'./bin/pyspark', 1), (u'either', 1), (u'Learning', 2), (u'small', 1), (u'--class', 1), (u'From', 1), (u'JDK', 1), (u'Scala', 1), (u'scala-lang.org', 1), (u'are', 1), (u'assembly', 2), (u'for', 4), (u'scala', 1), (u'libraries', 1), (u'Spark\".', 1), (u'minimal', 1), (u'./bin/spark-submit', 1), (u'run', 3), (u'book.', 1), (u'mini-complete-example', 1), (u'job', 1), (u'dependencies', 2), (u'alone', 1), (u'spark', 1), (u'package', 2), (u'of', 4), (u'&', 1), (u'against', 1), (u'OR', 1), (u'stand', 1), (u'slightly', 1), (u'or', 2), (u'mvn', 1), (u'number', 1), (u'cd', 1), (u'../learning-spark-examples/target/scala-2.10/learning-spark-examples-assembly-0.0.1.jar', 1), (u'./src/python/[example]', 1), (u'\"Learning', 1), (u'your', 1), (u'long', 1), (u'java', 1), (u'Protobuf', 1), (u'files.', 1), (u'create', 1), (u'*', 6), (u'been', 1), (u'[![Learning', 1), (u'These', 2), (u'R', 1), (u'be', 1), (u'copy', 1), (u'with', 4), (u'than', 1), (u'===', 2), (u'versions', 2), (u'require', 2), (u'can', 2), (u'install', 2), (u'example', 1), (u'examples', 4), (u'and', 3), (u'1.7', 1), (u'file', 1), (u'1.3', 2), (u'sudo', 1), (u'Submit', 1), (u'an', 1), (u'spark-submit', 1), (u'as', 1), (u'have', 3), (u'in', 2), (u'You', 1), (u'com.oreilly.learningsparkexamples.[lang].[example]', 1), (u'different', 1), (u'Python', 2), (u'-', 2), (u'also', 2), (u'build', 2), (u'you', 1), (u'Imap', 1), (u'higher', 1), (u'added', 1), (u'may', 1), (u'urllib3', 1), (u'Spark](http://akamaicovers.oreilly.com/images/0636920028512/cat.gif)](http://www.jdoqocy.com/click-7645222-11260198?url=http%3A%2F%2Fshop.oreilly.com%2Fproduct%2F0636920028512.do%3Fcmp%3Daf-strata-books-videos-product_cj_9781449358600_%2525zp&cjsku=0636920028512)', 1), (u'running', 1), (u'such', 1), (u'The', 1), (u'debian', 1), (u'a', 3), (u'2.10.3', 1), (u'required', 1), (u'script', 1), (u'so', 1), (u'Examples', 2), (u'the', 10), (u'protobuf-compiler', 1)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wordCounts.cache()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 71,
       "text": [
        "PythonRDD[274] at collect at <ipython-input-68-cc58a8ed4877>:8"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print wordCounts.collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u'===============', 1), (u'all', 1), (u'code', 1), (u'Requirements', 1), (u'just', 1), (u'CRAN', 1), (u'We', 1), (u'jar', 1), (u'to', 1), (u'$SPARK_HOME;', 1), (u'directory.', 1), (u'On', 1), (u'updated', 1), (u'./sbt/sbt', 1), (u'==', 1), (u'they', 1), (u'ChapterSixExample', 1), (u'apt-get', 1), (u'Spark', 5), (u'compiler', 1), (u'./bin/pyspark', 1), (u'either', 1), (u'Learning', 2), (u'small', 1), (u'--class', 1), (u'From', 1), (u'JDK', 1), (u'Scala', 1), (u'scala-lang.org', 1), (u'are', 1), (u'assembly', 2), (u'for', 4), (u'scala', 1), (u'libraries', 1), (u'Spark\".', 1), (u'minimal', 1), (u'./bin/spark-submit', 1), (u'run', 3), (u'book.', 1), (u'mini-complete-example', 1), (u'job', 1), (u'dependencies', 2), (u'alone', 1), (u'spark', 1), (u'package', 2), (u'of', 4), (u'&', 1), (u'against', 1), (u'OR', 1), (u'stand', 1), (u'slightly', 1), (u'or', 2), (u'mvn', 1), (u'number', 1), (u'cd', 1), (u'../learning-spark-examples/target/scala-2.10/learning-spark-examples-assembly-0.0.1.jar', 1), (u'./src/python/[example]', 1), (u'\"Learning', 1), (u'your', 1), (u'long', 1), (u'java', 1), (u'Protobuf', 1), (u'files.', 1), (u'create', 1), (u'*', 6), (u'been', 1), (u'[![Learning', 1), (u'These', 2), (u'R', 1), (u'be', 1), (u'copy', 1), (u'with', 4), (u'than', 1), (u'===', 2), (u'versions', 2), (u'require', 2), (u'can', 2), (u'install', 2), (u'example', 1), (u'examples', 4), (u'and', 3), (u'1.7', 1), (u'file', 1), (u'1.3', 2), (u'sudo', 1), (u'Submit', 1), (u'an', 1), (u'spark-submit', 1), (u'as', 1), (u'have', 3), (u'in', 2), (u'You', 1), (u'com.oreilly.learningsparkexamples.[lang].[example]', 1), (u'different', 1), (u'Python', 2), (u'-', 2), (u'also', 2), (u'build', 2), (u'you', 1), (u'Imap', 1), (u'higher', 1), (u'added', 1), (u'may', 1), (u'urllib3', 1), (u'Spark](http://akamaicovers.oreilly.com/images/0636920028512/cat.gif)](http://www.jdoqocy.com/click-7645222-11260198?url=http%3A%2F%2Fshop.oreilly.com%2Fproduct%2F0636920028512.do%3Fcmp%3Daf-strata-books-videos-product_cj_9781449358600_%2525zp&cjsku=0636920028512)', 1), (u'running', 1), (u'such', 1), (u'The', 1), (u'debian', 1), (u'a', 3), (u'2.10.3', 1), (u'required', 1), (u'script', 1), (u'so', 1), (u'Examples', 2), (u'the', 10), (u'protobuf-compiler', 1)]\n"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pythonLines.cache()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 75,
       "text": [
        "PythonRDD[275] at RDD at PythonRDD.scala:43"
       ]
      }
     ],
     "prompt_number": 75
    }
   ],
   "metadata": {}
  }
 ]
}