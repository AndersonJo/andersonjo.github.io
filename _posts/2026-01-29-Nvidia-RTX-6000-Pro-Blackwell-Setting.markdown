---
layout: post
title:  "Nvidia RTX 6000 Pro Blackwell Workstation Settings"
date:   2026-01-29 01:00:00
categories: "format"
asset_path: /assets/images/
tags: ['pytorch', 'tensorflow', 'cuda']
---

# 1. Install Easy CUDA

We're going to install Pytorch on CUDA-13.0<br> 
you need to run both cu12 and cu13.<br>
just run all of them below. <br>

```bash
# for CUDNN 12 & 13
pip install nvidia-cudnn-cu12 \
            nvidia-cublas-cu12 \
            nvidia-cufft-cu12 \
            nvidia-curand-cu12 \
            nvidia-cusolver-cu12 \
            nvidia-cusparse-cu12 \
            nvidia-nccl-cu12

# for CUDNN 13
pip install nvidia-cudnn-cu13 \
            nvidia-nccl-cu13
```


modify ~/.bashrc

NVIDIA_HOME is different, depending on your python version


```bash
# CUDA
NVIDIA_HOME="$HOME/.pyenv/versions/3.12.10/lib/python3.12/site-packages/nvidia"

# 2. ÌïÑÏöîÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨ Í≤ΩÎ°úÎì§ÏùÑ LD_LIBRARY_PATHÏóê Ï∂îÍ∞Ä
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:${NVIDIA_HOME}/cudnn/lib
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:${NVIDIA_HOME}/cublas/lib
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:${NVIDIA_HOME}/cufft/lib
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:${NVIDIA_HOME}/curand/lib
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:${NVIDIA_HOME}/cusolver/lib
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:${NVIDIA_HOME}/cusparse/lib
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:${NVIDIA_HOME}/nccl/lib
```




# 2. Install Pytorch

It's better to install Pytorch preview (nightly) version.<br>
I use CUDA Version 13.1

go to https://pytorch.org and find your matched version. 
 - PyTorch Build: Preview (Nightly)
 - CUDA 13.0


```bash
#$ pip install --pre torch torchvision --index-url https://download.pytorch.org/whl/nightly/cu130
$ pip install torch torchvision --index-url https://download.pytorch.org/whl/cu130

# check pytorch version
$ python -c "import torch; print(torch.__version__)"
```

here you can test it like this

```python
import time
import os

# Suppress TF logs for cleaner output
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 

def test_pytorch():
    print("--- PyTorch Check ---")
    import torch
    
    if torch.cuda.is_available():
        device = torch.device("cuda")
        gpu_name = torch.cuda.get_device_name(0)
        vram = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"‚úÖ GPU Detected: {gpu_name} ({vram:.2f} GB VRAM)")
        
        # Simple Compute Test
        x = torch.randn(5000, 5000, device=device)
        y = torch.randn(5000, 5000, device=device)
        start = time.time()
        result = torch.matmul(x, y)
        torch.cuda.synchronize() # Wait for compute to finish
        print(f"‚úÖ Matrix Mul (5k x 5k) Time: {time.time() - start:.4f}s")
    else:
        print("‚ùå PyTorch cannot see the GPU.")

test_pytorch()
```




# 2. Install Unsloth

- my conclusion, Unsloth doesn't work with Tensorflow Nightly version. 
- I decided not to install tensorflow in system level (I will use Tensorflow in virtualenv)


```bash
$ pip install unsloth
#$ pip install tf-keras transformers --no-deps

# must install it again to upgrade torch and torchvision. 
# as of this writing, torch==2.10.0, torchao==0.15.0, torchaudio==2.9.1, torchvision==0.25.0
# these versions work well with unsloth==2026.1.4, unsloth_zoo==2026.1.4
$ pip install --upgrade torch torchvision
```

ÌÖåÏä§Ìä∏ 

```python
import torch
from unsloth import FastLanguageModel

def test_unsloth():
    # ÏÑ§Ï†ï
    max_seq_length = 2048
    dtype = None # NoneÏúºÎ°ú ÏÑ§Ï†ï Ïãú ÏûêÎèôÏúºÎ°ú bfloat16 (RTX 6000 ÏßÄÏõê) Í∞êÏßÄ
    load_in_4bit = True # UnslothÏùò ÌïµÏã¨Ïù∏ 4bit QLoRA Î°úÎî© ÌÖåÏä§Ìä∏
    
    print(f"üîπ GPU Check: {torch.cuda.get_device_name(0)}")
    print(f"üîπ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")

    # 1. Î™®Îç∏ Î∞è ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú ÌÖåÏä§Ìä∏
    print("\n[1/3] Loading Llama-3.2-1B model...")
    try:
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name = "unsloth/Llama-3.2-1B-Instruct", 
            max_seq_length = max_seq_length,
            dtype = dtype,
            load_in_4bit = load_in_4bit,
        )
        print("‚úÖ Model loaded successfully.")
    except Exception as e:
        print(f"‚ùå Model load failed: {e}")
        return

    # 2. Inference ÌÖåÏä§Ìä∏ (FastLanguageModel ÏµúÏ†ÅÌôî ÎèôÏûë ÌôïÏù∏)
    print("\n[2/3] Running Inference...")
    FastLanguageModel.for_inference(model) # Native 2x faster inference
    
    inputs = tokenizer(
        [
            "unsloth ÎùºÏù¥Î∏åÎü¨Î¶¨Ïùò Ï£ºÏöî Ïû•Ï†êÏùÄ Î¨¥ÏóáÏù∏Í∞ÄÏöî? ÏßßÍ≤å ÏöîÏïΩÌï¥Ï£ºÏÑ∏Ïöî."
        ], return_tensors = "pt"
    ).to("cuda")

    outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)
    decoded_output = tokenizer.batch_decode(outputs)[0]
    
    # Í≤∞Í≥º Ï∂úÎ†• (ÎÇ¥Ïö©Î≥¥Îã§Îäî ÏóêÎü¨ ÏóÜÏù¥ ÏÉùÏÑ±ÎêòÏóàÎäîÏßÄÍ∞Ä Ï§ëÏöî)
    print("‚úÖ Inference completed.")
    
    # 3. LoRA Ïñ¥ÎåëÌÑ∞ Î∂ÄÏ∞© ÌÖåÏä§Ìä∏ (ÌïôÏäµ Ï§ÄÎπÑ ÏÉÅÌÉú ÌôïÏù∏)
    print("\n[3/3] Testing LoRA Adapter attachment...")
    try:
        model = FastLanguageModel.get_peft_model(
            model,
            r = 16,
            target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"],
            lora_alpha = 16,
            lora_dropout = 0,
            bias = "none",
            use_gradient_checkpointing = True,
        )
        print(f"‚úÖ LoRA Adapters attached. Trainable parameters: {model.print_trainable_parameters()}")
    except Exception as e:
        print(f"‚ùå LoRA attachment failed: {e}")

test_unsloth()
```



# 3. Install Tensorflow

 - you need to install latest version of tensorflow "tf-nightly" with cuda option.
 - Also it doesn't work with `Unsloth` well.
 - I decided not to install Tensorflow nightly version on system
 - instead, I will use tensorflow in virtualenv

```bash
# Create tensorflow env
$ pyenv virtualenv tensroflow-nightly
$ pyenv activate tensroflow-nightly

# Install tensorflow nightly version with cuda support
$ pip install "tf-nightly[and-cuda]"
#
# ÌÖåÏä§Ìä∏
$ python -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
```

modify ~/.bashrc

```bash
# CUDA for Tensorflow
NVIDIA_HOME="$HOME/.pyenv/versions/3.12.10/lib/python3.12/site-packages/nvidia"

# 2. ÌïÑÏöîÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨ Í≤ΩÎ°úÎì§ÏùÑ LD_LIBRARY_PATHÏóê Ï∂îÍ∞Ä
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:${NVIDIA_HOME}/cudnn/lib
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:${NVIDIA_HOME}/cublas/lib
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:${NVIDIA_HOME}/cufft/lib
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:${NVIDIA_HOME}/curand/lib
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:${NVIDIA_HOME}/cusolver/lib
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:${NVIDIA_HOME}/cusparse/lib
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:${NVIDIA_HOME}/nccl/lib
```



here's a bit more complex test. 

```python
import time
import os

def test_tensorflow():
    print("\n--- TensorFlow Check ---")
    import tensorflow as tf
    
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        print(f"‚úÖ GPU Detected: {len(gpus)} device(s)")
        for gpu in gpus:
            print(f"   - {gpu.device_type}: {gpu.name}")
            
        # Simple Compute Test
        with tf.device('/GPU:0'):
            a = tf.random.normal([5000, 5000])
            b = tf.random.normal([5000, 5000])
            start = time.time()
            c = tf.matmul(a, b)
            _ = c.numpy() # Force execution
            print(f"‚úÖ Matrix Mul (5k x 5k) Time: {time.time() - start:.4f}s")
    else:
        print("‚ùå TensorFlow cannot see the GPU.")


test_tensorflow()
```



# 4. Install vLLM

**WARNING! here we can't install both torch and vllm at the same time!**

when you need to run, you need to run in virtualenv. 
if you install both torch and vllm, vllm downgrade your torch -> the downgraded torch will not work on RTX 6000 PRO.

```bash
# Run in virtualenv
$ pyenv virtualenv 3.12.10 vllm
$ pyenv activate vllm
$ pip install vllm 

```

```bash
python -m vllm.entrypoints.openai.api_server \
    --model openai/gpt-oss-20b \
    --tensor-parallel-size 1 \
    --gpu-memory-utilization 0.5 \
    --trust-remote-code \
    --max-model-len 5096
```




# Stable Diffusion WebUI


```bash
$ export STABLE_DIFFUSION_REPO=https://github.com/joypaul162/Stability-AI-stablediffusion.git
$ ./webui.sh
```
