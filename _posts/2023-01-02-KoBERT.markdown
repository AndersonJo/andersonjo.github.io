---
layout: post
title:  "Personal experiments in BERT"
date:   2023-01-02 01:00:00
categories: "nlp"
asset_path: /assets/images/
tags: ['kobert', 'electra']
---

# 1. 여러가지 실험

## KoBERT

- [Cosine Similarity 를 fine-tuning 없이 해보기](https://github.com/AndersonJo/nlp-anderson/blob/master/301%20KoBERT/01%20Cosine%20Similarity%20without%20fine%20tuning.ipynb)
  - fine tuning 없이 두개의 문장을 넣고 얼마나 일치하는지 알아본 실험
  - 결론적으로는 fine tuning 은 반드시 필요하다
