---
layout: post
title:  "Spark 101"
date:   2016-02-11 01:00:00
categories: "spark"
static: /assets/posts/Spark101/
tags: ['SBT', 'Scala', 'PySpark']

---

<img src="{{ page.static }}math.jpg" class="img-responsive img-rounded">

# Installation

#### <span class="glyphicon glyphicon-minus" aria-hidden="true"></span> Installing SBT on Ubuntu

{% highlight bash %}
echo "deb https://dl.bintray.com/sbt/debian /" | sudo tee -a /etc/apt/sources.list.d/sbt.list
sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 642AC823
sudo apt-get update
sudo apt-get install sbt
{% endhighlight %}

#### <span class="glyphicon glyphicon-minus" aria-hidden="true"></span> Download Apache Spark

[다운로드 페이지][download]에서 Spark를 다운 받습니다.

#### <span class="glyphicon glyphicon-minus" aria-hidden="true"></span> Python Path Configuration

적절한곳에 압축을 풀어주고, 설치를 합니다.<br>
(아래의 SPARK_HOME은 스파크가 설치할 곳으로 수정해주셔야 합니다.)

{% highlight bash %}
export SPARK_HOME=/home/anderson/apps/spark-1.6.2-bin-hadoop2.6
export PYTHONPATH=$PYTHONPATH:$SPARK_HOME/python
export PYTHONPATH=$PYTHONPATH:$SPARK_HOME/python/lib/py4j-0.9-src.zip

export CLASSPATH=$CLASSPATH:/path/postgresql-9.3-1103.jdbc3.jar
export SPARK_CLASSPATH=/path/postgresql-9.3-1103.jdbc3.jar
{% endhighlight %}


설치가 잘 되었다면, import pyspark가 에러없이 import가 됩니다.

{% highlight python %}
import pyspark
{% endhighlight %}


#### <span class="glyphicon glyphicon-minus" aria-hidden="true"></span> Pycharm Configuration

Project Structure에서 PySpark가 있는 위치를 Add Content Root를 눌러서 추가시켜줍니다.

<img src="{{ page.static }}pycharm.png" class="img-responsive img-rounded">


#### <span class="glyphicon glyphicon-minus" aria-hidden="true"></span> IntelliJ Configuration

Project생성시에는 Scalar -> SBT 를 선택합니다. <br>
Project Structure -> Modules -> Dependencies -> + 추가 -> $SPARK_HOME/lib 를 추가<br>

<img src="{{ page.static }}intellij_project_structure.png" class="img-responsive img-rounded">




# Run Spark Server

{% highlight bash %}
cd $SPARK_HOME/sbin
./start-master.sh -h hostname
./start-slaves.sh
{% endhighlight %}

http://localhost:8080/ 에 들어가서 master Web UI를 확인해볼수 있습니다.<br>
기본 Listening port는 **7077** 입니다.<br>
이때 hostname에다가 0.0.0.0을 쓰면 안됩니다. (Slave 추가할때 에러남)


| Argument | Meaning |
|:---------|:--------|
| -h HOST, --host HOST | Hostname to listen on | 
|-i HOST, --ip HOST | Hostname to listen on (deprecated, use -h or --host) |
|-p PORT, --port PORT | Port for service to listen on (default: 7077 for master, random for worker) |
|--webui-port PORT | Port for web UI (default: 8080 for master, 8081 for worker) |
|-c CORES, --cores CORES | Total CPU cores to allow Spark applications to use on the machine<br>(default: all available); only on worker |
|-m MEM, --memory MEM | Total amount of memory to allow Spark applications to use on the machine, <br>in a format like 1000M or 2G (default: your machine's total RAM minus 1 GB);<br>only on worker |
|-d DIR, --work-dir DIR | Directory to use for scratch space and job output logs <br>(default: SPARK_HOME/work); only on worker |
|--properties-file FILE | Path to a custom Spark properties file to load (default: conf/spark-defaults.conf) |

<img src="{{ page.static }}spark_web.png" class="img-responsive img-rounded">

# Scala Spark


#### <span class="glyphicon glyphicon-minus" aria-hidden="true"></span> build.sbt

중요한 점은, scalaVersion이 정확하게 2.10.5이어야 합니다.

{% highlight bash %}
name := "ScalaTutorial"
version := "1.0"
scalaVersion := "2.10.5"
libraryDependencies += "org.apache.spark" % "spark-core_2.11" % "1.6.2"

{% endhighlight %}


* 참고: [Apache Spark Maven Repository][Apache Spark Maven Repository]

# PySpark Basic 101

#### <span class="glyphicon glyphicon-minus" aria-hidden="true"></span> Initializing a SparkContext

{% highlight python %}
from pyspark import SparkContext, SparkConf

spconf = SparkConf().setMaster('local').setAppName('Tutorial')
sc = SparkContext(conf=spconf)
{% endhighlight %}

실행시키는 방법은 Spark home안의 bin 디렉토리안에있는 spark-submit을 이용합니다.

{% highlight bash %}
spark-submit tutorial.py
{% endhighlight %}


#### <span class="glyphicon glyphicon-minus" aria-hidden="true"></span> Word Count

Hadoop Map-Reduce에서도 Word Count부터 했는데, 마찬가지로 Spark에서도 Word Count부터 시작을 해보겠습니다.

{% highlight python %}
textFile = sc.textFile('README.md')
print textFile.count()  # 36 ;the number of lines
print textFile.first()  # Examples for Learning Spark

pythonLines = textFile.filter(lambda line: 'python' in line.lower())
print pythonLines.first()
{% endhighlight %}

#### <span class="glyphicon glyphicon-minus" aria-hidden="true"></span> Map Reduce

가장 많은 단어가 들어있는 line의 단어 갯수를 알아냅니다.

{% highlight python %}
textFile = sc.textFile('README.md')
def max(a, b):
    return a if a > b else b

textFile.map(lambda line: len(line.split(' '))).reduce(max) # 36
{% endhighlight %}

단어별 나온 횟수를 알아냅니다.<br>
**flatMap**은 받은 element를 리스트같은 형태로 lambda에서 리턴시키면 flatten시켜버립니다.<br>
즉 flatMap 다음에 다시 map함수가 나오는데.. 여기서 flatted 된 각각 elements를 map을 돌 수 있습니다.

**reduceByKey**는 Merge의 개념이라고 보면 됩니다.

{% highlight python %}
wordCounts = textFile.flatMap(lambda line: line.split()).\
    map(lambda word: (word, 1)).reduceByKey(combine)
print sorted(wordCounts.collect(), key=lambda d: d[1], reverse=True)
# [(u'the', 10), (u'*', 6), (u'Spark', 5), (u'for', 4), (u'of', 4), ...]
{% endhighlight %}

#### <span class="glyphicon glyphicon-minus" aria-hidden="true"></span> Cache or Persist

기본적으로 Spark는 action을 실행시키면 (즉 lazy하게 실행이 저장했다가 실제로 분석을 시작하는 때) 실행시킬때 마다 **recompute**하게 됩니다.
만약에 나온 결과물을 재사용하고 싶다면,  cache() 또는 persist()함수를 사용할 수 있습니다.

cache() 또는 persist()를 사용하게 되면 여러대의 분산된 컴퓨터에 동일한 데이터가 메모리상에 존재하게 되고,<br>
재사용시 메모리에 저장된 결과물을 재사용하게 됩니다.

* cache는 메모리에, persist는 어디 위치에 저장할지 정할 수 있습니다.

{% highlight python %}
rdd.cache()
{% endhighlight %}

{% highlight python %}
from pyspark import StorageLevel
...
rdd.persist(StorageLevel.MEMORY_ONLY_SER_2)
{% endhighlight %}


|Storage Level | Meaning|
|:-------------|:-------|
| MEMORY_ONLY |Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they're needed. This is the default level.|
|MEMORY_AND_DISK |Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that don't fit on disk, and read them from there when they're needed.|
|MEMORY_ONLY_SER |Store RDD as serialized Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a fast serializer, but more CPU-intensive to read.|
|MEMORY_AND_DISK_SER |Similar to MEMORY_ONLY_SER, but spill partitions that don't fit in memory to disk instead of recomputing them on the fly each time they're needed.|
|DISK_ONLY |Store the RDD partitions only on disk.|
|MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc |Same as the levels above, but replicate each partition on two cluster nodes.|



#### <span class="glyphicon glyphicon-minus" aria-hidden="true"></span> RDD Operations

RDD에는 2가지 type의 operations이 있습니다. transformation과 action.<br>
**transformation**은 새로운 RDD를 리턴시킵니다. (즉 map, reduce 같은것..)<br>
**action**은 우리가 만들어 놓은 RDD를 실제로 kick off시켜서 결과를 내놓습니다. (count, first 같은 함수들..)

#### <span class="glyphicon glyphicon-minus" aria-hidden="true"></span> Union & Lineage Graph

만약 Transformation으로 새로운 RDD를 얻게 되면, Spark는 해당 RDD의 계보(Lineage)를 keep track of하게 됩니다.<br>
만약 transformation으로 손실된 데이터가 있다면, Lineage에 따라서 Union을 통해 서로 다시 합칠수도 있습니다.

{% highlight python %}
data = sc.parallelize(['a', 'b', 'c', 'd', 'e', 1,2,3,4,5])
data.cache()

alpha = data.filter(lambda word: type(word) == str or type(word) == unicode)
alpha.cache()

numeric = data.filter(lambda word: type(word) == int)
numeric.cache()

print alpha.collect() #  ['a', 'b', 'c', 'd', 'e']
print numeric.collect() # [1, 2, 3, 4, 5]

print alpha.union(numeric).collect() # ['a', 'b', 'c', 'd', 'e', 1, 2, 3, 4, 5]
{% endhighlight %}

#### <span class="glyphicon glyphicon-minus" aria-hidden="true"></span> Take VS Collect

Take는 전체 데이터셋에서 일부를 가져오고, Collect는 전체 데이터를 driver (현재 돌리고 있는 컴퓨터)로 가져옵니다.<br>
collect함수는 전체 데이터를 가져오기 때문에 Map-Reduce로 작은 싸이즈로 줄어들었거나, 또는 driver의 메모리안으로 가져올 만큼의 양으로
줄어들었을때 가능합니다.

이렇게 큰 경우 saveAsTextFile() 또는 saveAsSequenceFile()을 사용해서 저장하거나, S3, HDFS에 저장할 수 있습니다.

{% highlight python %}
rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])
for line in rdd.take(5):
    print line # 1 2 3 4 5
{% endhighlight %}

#### <span class="glyphicon glyphicon-minus" aria-hidden="true"></span> Top

순서상이 아니라, 값이 가장 큰것 순서대로 리턴을 시킵니다.

{% highlight python %}
rdd = sc.parallelize(['a', 'b', 'c', 'd', 'e', 'z', 'y', 'x', 'f', 'g'])
rdd.top(3) # ['z', 'y', 'x']
{% endhighlight %}


#### <span class="glyphicon glyphicon-minus" aria-hidden="true"></span> Set

union, intersection, subtract등의 집합 연산도 가능합니다.

{% highlight python %}
rdd1 = sc.parallelize([1,2,3,4,5, 'a'])
rdd2 = sc.parallelize(['a', 'b', 'c', 1, 2])

rdd1.union(rdd2).collect() # [1, 2, 3, 4, 5, 'a', 'a', 'b', 'c', 1, 2]
rdd1.intersection(rdd2).collect() # ['a', 2, 1]
rdd1.subtract(rdd2).collect() # [4, 3, 5]
{% endhighlight %}

Cartesian Product는 A, B RDD에 있는 모든 elements들을 pair로 묶습니다.<br>
유사성을 검사할때 좋지만, 매우 expensive 하니 크기가 큰 RDD에는 적합하지 않습니다.

{% highlight python %}
rdd1 = sc.parallelize([1,2])
rdd2 = sc.parallelize(['a', 'b', 'c'])

rdd1.cartesian(rdd2).collect()
# [(1, 'a'), (1, 'b'), (1, 'c'), (2, 'a'), (2, 'b'), (2, 'c')]
{% endhighlight %}


#### <span class="glyphicon glyphicon-minus" aria-hidden="true"></span> Distinct & Sample

Distinct는 중복되는 elements를 제거합니다.

{% highlight python %}
rdd = sc.parallelize([1,1,2,2,2,3,4])
rdd.distinct().collect() # [1, 2, 3, 4]
{% endhighlight %}

**rdd.sample(self, withReplacement, fraction, seed=None)**<br>
**withReplacement**가 True이면 중복해서 여러번 같은 element를 선택할 수 있게 됩니다.<br>
**fraction** sample의 싸이즈를 정합니다. 0~1사이의 소수값이 들어가고,<br>
값이 크면 클수록 sample의 싸이즈또한 커지게 됩니다.

{% highlight python %}
rdd = sc.parallelize([10,20,30,40,50,60,70,80,90,100])
rdd.sample(False, 0.5).collect()
# [40, 50, 60]
# [30, 40, 50, 90, 100]
# [50, 80, 90, 100]

rdd.sample(True, 0.5).collect()
# [10, 10, 20, 30, 60, 60, 80, 90, 100, 100]
# [10, 100, 100]

rdd.sample(True, 0).collect()
# []
{% endhighlight %}


#### <span class="glyphicon glyphicon-minus" aria-hidden="true"></span> Aggregate

aggregate는 reduce와 유사하지만, Return Value가 다른 타입입니다.

**rdd.aggregate(self, zeroValue, seqOp, combOp)**
**zeroValue** 는 초기값

아래는 평균값을 구하는 코드입니다.<br>
첫번째 **lambda acc, v**에서 리턴시 전혀 다른 type으로 리턴가능하며, acc는 변경가능하지만, v값은 변경하면 안됩니다.<br>
두번째 **lambda acc1, acc2**에서는 첫번째 lambda에서 돌은 값을 merge시켜주는 역활을 합니다.

{% highlight python %}
rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10])
sum, avg = rdd.aggregate((0, 0),
              lambda acc, v: (acc[0] + v, acc[1] + 1),
              lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))
# sum: 55, avg: 10
print sum/float(avg) # 5.5
{% endhighlight %}


# SQL & Dataframe

#### Initialization


.bashrc에 SPARK_CLASSPATH에다가 JDBC jar파일을 추가해줍니다.
{% highlight bash %}
export CLASSPATH=$CLASSPATH:/path/postgresql-9.3-1103.jdbc3.jar
export SPARK_CLASSPATH=/path/postgresql-9.3-1103.jdbc3.jar
{% endhighlight %}


{% highlight python %}
from pyspark import SparkContext, SparkConf
from pyspark.sql import SQLContext

spconf = SparkConf().setMaster('local').setAppName('Tutorial')
sc = SparkContext(conf=spconf)
sqlContext = SQLContext(sc)
{% endhighlight %}

SQLContext 이외에도 HiveContext 를 이용할수도 있습니다.

#### Creating Dataframe

{% highlight python %}
df = sqlContext.read.json("examples/src/main/resources/people.json")
df.show()
# +----+-------+
# | age|   name|
# +----+-------+
# |null|Michael|
# |  30|   Andy|
# |  19| Justin|
# +----+-------+
{% endhighlight %}

{% highlight python %}
df.printSchema()
# root
#  |-- age: long (nullable = true)
#  |-- name: string (nullable = true)
{% endhighlight %}


{% highlight python %}
df.select("name").show()
# +-------+
# |   name|
# +-------+
# |Michael|
# |   Andy|
# | Justin|
# +-------+
{% endhighlight %}


{% highlight python %}
df.select(df['name'], df['age'] + 1).show()
# +-------+---------+
# |   name|(age + 1)|
# +-------+---------+
# |Michael|     null|
# |   Andy|       31|
# | Justin|       20|
# +-------+---------+
{% endhighlight %}


{% highlight python %}
df.filter(df['age'] > 21).show()
# +---+----+
# |age|name|
# +---+----+
# | 30|Andy|
# +---+----+
{% endhighlight %}

{% highlight python %}
df.groupBy("age").count().show()
# +----+-----+
# | age|count|
# +----+-----+
# |null|    1|
# |  19|    1|
# |  30|    1|
# +----+-----+
{% endhighlight %}


# SQL using JDBC

#### PostgreSQL Connection

{% highlight python %}
import os
os.environ['SPARK_CLASSPATH'] = "/path/to/postgresql-9.3-1103.jdbc3.jar"

df = sqlContext.read.format("jdbc").options(
    url="jdbc:postgresql://host:port/database",
    port=5432,
    driver = "org.postgresql.Driver",
    dbtable = "users",
    user="user_name",
    password="password").load()
{% endhighlight %}

#### Example

{% highlight python %}
sqlContext.registerDataFrameAsTable(df, 'users')
df2 = sqlContext.sql('select id, nickname from users where id=155 limit 1')
print df2.collect()
# [Row(id=155, nickname=u'\uc575\uadf8\ub9ac\uc564\ub354\uc2a8')]
{% endhighlight %}





[download]: http://spark.apache.org/downloads.html
[Apache Spark Maven Repository]: https://mvnrepository.com/artifact/org.apache.spark/spark-core_2.10