---
layout: post
title:  "Recurrent Neural Network with Python"
date:   2016-06-25 01:00:00
categories: "machine-learning"
asset_path: /assets/posts/RNN-With-Python/
tags: ['LSTM', 'tanh', 'hyperbolic tangent', 'cross-entropy loss']

---

<div>
    <img src="{{ page.asset_path }}brain.png" class="img-responsive img-rounded" style="width:100%">
</div>

http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/

# Language Modeling

### Installing NLTK

{% highlight bash %}
sudo pip install numpy
sudo pip install nltk
{% endhighlight %}

{% highlight python %}
import nltk
nltk.download('punkt')
{% endhighlight %}

### Word Probability

이전 문장에 기반하여, 다음 어떤 글자가 올 확률이 높을까? 이런 문제는 다음의 공식을 보면 됩니다. 

$$ P(w1, ..., w_m) = \prod_{i=1} P(w_i | w_1, ..., w_{i-1}) $$

예를 들어서 "In Japan everything is CUTE" 라는 문장의 확률은, "In Japan everything is"라는 단어들이 주어졌을때, CUTE가 올 확률입니다. 
즉 $$ P( \text{"In Japan everything is CUTE"} ) $$ 의 확률은<br> 
$$ P( \text{CUTE} \ | \ \text{In Japan everything is}) $$ * <br>
$$ P( \text{is} \ | \ \text{In Japan everything}) $$ * <br> 
$$ P( \text{everything} \ | \ \text{In Japan}) $$ 이렇게 계속 반복..

위 모델링의 문제점은 (Bayes 확률이 그러하듯) 이전의 모든 단어들에 조건이 들어가기 때문에, 너무나 많은 컴퓨팅 파워와 메모리를 요구하며,
이는 현실적으로 거의 불가능한 수준의 모델링입니다. (위의 모델링은 제가 이전에 쓴 Bayes를 이용한 스팸필터링에서도 
너무나 많은 컴퓨팅 파워를 요구하기 때문에, Naive Bayes라는 것으로 대체를 하기도 했습니다.)
따라서 현실적으로 위의 공식보다는 RNN을 사용하며, 이론적으로는 RNN은 위의 공식처럼 long-term dependencies를 캡쳐할수 있습니다. 
(하지만 실제는 RNN또한 앞의 몇 단어정도를 캡쳐하는데 그치며, 제대로 하려면 LSTM이 필요합니다.)

# RNN 101

<img src="{{ page.asset_path }}rnn.jpg" class="img-responsive img-rounded">


#### $$ x_t $$는 a vector이고, $$ x $$ 는 matrix가 됩니다.


#### $$ s_t = tanh(U_{x_{t}} + W_{s_{t-1}}) $$




# Training Data And PreProcessing

[Reddit Comment on Google BigQuery][Reddit Comment on Google BigQuery] 에서 Reddit Comments를 다운 받을수 있습니다.

### Tokenize Text

>"He left!" ---> "He", "left", "!" 

일단 Reddit Comments를 다운받으면, Raw text는 있지만, 분석을 위해서는 word단위로 잘라줘야 합니다.
즉 comments를 sentences별로, sentences는 단어 단위로 tokenize가 필요합니다. 
이때 punctuation (!)의 경우는 따로 빼줘야 합니다. 예를 들어서 "He left!" 의 경우는 "He", "left", "!" 이렇게 빠져야 합니다.
Tokenization은 [Python NLTK Library][NLTK Library]의 word_tokenize 그리고 sent_tokenize함수를 통해서 쉽게 해결될수 있습니다. 

문장의 처음 시작과, 끝을 알기 위해서 SENTENCE_START, SENTENCE_END를 사용합니다.
또한 RNN에서는 String이 아닌 vectors를 사용하기 때문에 숫자값으로 바꿔줘야 합니다.

> [0, 179, 341, 416] --> [179, 341, 416, 255]

0은 SENTENCE_START를 가르키며, 그 다음에 나올 단어를 예측해 나온값이 255이며, 한칸씩 shift해서 결과값에서는 0이 빠져있습니다.


### Initialization

{% highlight python %}
class RNNNumpy(object):
    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):
        self.word_dim = word_dim
        self.hidden_dim = hidden_dim
        self.bptt_truncate = bptt_truncate
        self.U = np.random.uniform(-1. / np.sqrt(word_dim), 1. / np.sqrt(word_dim), (hidden_dim, word_dim))
        self.V = np.random.uniform(-1. / np.sqrt(word_dim), 1. / np.sqrt(word_dim), (word_dim, hidden_dim))
        self.W = np.random.uniform(-1. / np.sqrt(word_dim), 1. / np.sqrt(word_dim), (hidden_dim, word_dim))
{% endhighlight %}

### Forward Propagation

{% highlight python %}
def forward_propagation(self, x):
    # The total number of time steps
    T = len(x)

    # During forward propagation we save all hidden states in s because need them later.
    # We add one additional element for the initial hidden, which we set to 0
    s = np.zeros((T + 1, self.hidden_dim))

    # The outputs at each time step. Again, we save them for later.
    o = np.zeros((T, self.word_dim))
    for t in xrange(T):
        s[t] = self.U[:, x[t]]
        o[t] = self.softmax(self.V.dot(s[t]))
    return [o, s]
{% endhighlight %}

{% highlight python %}
VOCAB_SIZE = 10000

model = rnn.RNNNumpy(VOCAB_SIZE)
o, s = model.forward_propagation(x_train[9])
print 'Output:', o.shape
print ' State:', s.shape
print '  X train:', x_train[9]

Output: (13, 10000)
 State: (14, 100)
  X train: [0, 118, 14, 1603, 7, 92, 9999, 38, 6, 9999, 4787, 2680, 2]

{% endhighlight %}

# Mathmatics behind RNN

### Hyperbolic Tangent

$$ tanh(z) = \frac{sinh(z)}{cosh(z)} = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}} = \frac{e^{2z} - 1}{ e^{2z} + 1} $$

{% highlight python %}
data = np.arange(-3, 3, 0.1)
plot(data, np.apply_along_axis(np.tanh, axis=0, arr=data))
{% endhighlight %}

<img src="{{ page.asset_path }}tanh.png" class="img-responsive img-rounded">


{% highlight python %}
data2 = pd.Series([-1, -0.5, 0, 1, 1.5, 2])
data2.apply(np.tanh)
0   -0.761594
1   -0.462117
2    0.000000
3    0.761594
4    0.905148
5    0.964028
dtype: float64
{% endhighlight %}

{% highlight python %}
def tanh(x):
    return np.sinh(x)/np.cosh(x)
data2.apply(tanh)
0   -0.761594
1   -0.462117
2    0.000000
3    0.761594
4    0.905148
5    0.964028
dtype: float64
{% endhighlight %}

{% highlight python %}
def tanh2(x):
    return (np.e**(2*x) - 1)/(np.e**(2*x)+ 1)
data2.apply(tanh2)
0   -0.761594
1   -0.462117
2    0.000000
3    0.761594
4    0.905148
5    0.964028
dtype: float64
{% endhighlight %}


### Cross Entropy Loss

$$ L(y, o) = -\frac{1}{N} \sum{y_n \ log \ o_n} $$

### Cross-entropy loss in Logistic Regression 

$$ \mathcal{L}(X, Y) = -\frac{1}{n} \sum_{i=1}^n y^{(i)} \ln a(x^{(i)}) + \left(1 - y^{(i)}\right) \ln \left(1 - a(x^{(i)})\right) $$

* $$ X = \left\{x^{(1)},\dots,x^{(n)}\right\} $$ inputs값을 나타내며, 
$$ Y=\left\{y^{(1)},\dots,y^{(n)} \right\} $$

* $$ a(x) $$는 Neural Network에서 x값을 넣었을때 나오는 output입니다.

* $$ y^{(i)} $$는 0 또는 1입니다.


[Reddit Comment on Google BigQuery]: https://bigquery.cloud.google.com/dataset/fh-bigquery:reddit_comments
[NLTK Library]: http://www.nltk.org/