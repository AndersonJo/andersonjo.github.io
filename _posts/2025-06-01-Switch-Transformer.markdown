---
layout: post
title: "Switch Transformer - Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"
date: 2025-06-01 01:00:00
categories: "nlp"
asset_path: /assets/images/
tags: [ ]
---

# Switch Transformer - Scaling to Trillion Parameter Models with Simple and Efficient Sparsity

| Key              | Value                            |
|:-----------------|:---------------------------------|
| Paper            | https://arxiv.org/pdf/2101.03961 |
| publication Date | 2021                             |


# Problem

MoE (Mixture of Experts) ëª¨ë¸ì€ inputs ë§ˆë‹¤ ì„œë¡œë‹¤ë¥¸ parameters ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì—,
ì „ì²´ parameters ìˆ˜ëŠ” í­ë°œì ìœ¼ë¡œ ëŠ˜ì–´ë‚˜ë„, ì‹¤ì œ ì‚¬ìš©í•˜ëŠ” ë¶€ë¶„ì€ ì†Œìˆ˜ë¼ ì—°ì‚°ëŸ‰ì€ ì¼ì •í•©ë‹ˆë‹¤.

í•˜ì§€ë§Œ ì‹¤ì œ ì ìš©ì€ ì–´ë µìŠµë‹ˆë‹¤. 
- ê¸°ì¡´ MoE ëª¨ë¸ì˜ ë¬¸ì œì 
    - ë³µì¡í•œ Routing ì•Œê³ ë¦¬ì¦˜ì´ í•„ìš”
    - communication overheadê°€ í¼
    - í•™ìŠµ ë¶ˆì•ˆì •ì„± gradient exploding í˜„ìƒ ë°œìƒ
- Switch Transformer ëŠ” ì´ëŸ¬í•œ ê¸°ì¡´ì˜ ë¬¸ì œë“¤ì„ í•´ê²°í•˜ì˜€ìŒ. 

# Model 

í•µì‹¬ì€
 - Sparse Training 
 - ê¶ê·¹ì ìœ¼ë¡œ parameter ê°¯ìˆ˜ ìì²´ë¥¼ maximize í•˜ëŠ” ê²ƒ. (ë§¤ìš° íš¨ìœ¨ì ì¸ ë°©ì‹ìœ¼ë¡œ)
 - ì´ë ‡ê²Œ parameters ë¥¼ ëŠ˜ë¦¬ì§€ë§Œ, Floating point operations (FLOPs) ë¥¼ constant ê°’ìœ¼ë¡œ ë°”ê¾¸ëŠ” ê²ƒ. 
    - ì¦‰ parameters ëŠ” ëŠ˜ë¦¬ì§€ë§Œ, ì—°ì‚°ì€ constant í•˜ê²Œ ë¨.  

<img src="{{ page.asset_path }}switch_transformer_model.png" class="img-responsive img-rounded img-fluid center" style="border: 2px solid #333333">


## ê¸°ì¡´ MOE ëª¨ë¸ ë°©ì‹ - Mixture of Experts Routing

- MOE (Mixture of Experts) ê°œë…ì€ ì´ë¯¸ 2017ë…„ Shazeer et alì— ì˜í•´ ì œì•ˆë˜ì—ˆìŒ. 
- route ì— í•´ë‹¹í•˜ëŠ” $$ W_r $$ ê°’ì´ ì¡´ì¬í•˜ê³ , inputê°’ê³¼ ê³±í•´ì ¸ì„œ logitsì„ ìƒì„±í•¨ $$ h(x) = W_r \cdot x $$
- ì´í›„ì— softmaxë¥¼ ì ìš©í•˜ê³ , top-k ê°œì˜ experts ë¥¼ ì„ íƒí•¨. í¬ì¸íŠ¸ëŠ” ì—¬ëŸ¬ê°œì˜ expertsë¥¼ ì„ íƒí•¨

ì•„ë˜ëŠ” êµ¬ì²´ì ì¸ ê³µì‹

$$ h(x) = W_r \cdot x $$

- W_r : route matrix (ì´ê²Œ í•™ìŠµì´ ë˜ë©´ì„œ expertsë¥¼ ì„ íƒí•˜ëŠ” ì—­í™œì„ í•¨)
- x: input vector

ì´í›„ì— softmaxë¥¼ ì ìš©í•˜ì—¬, top-kê°œì˜ expertsë¥¼ ì„ íƒí•¨.

$$ p_i(x) = \frac{e^{h(x)_i}}{\sum^N_j e^{h(x)_j}} $$

ë§Œì•½ Tê°€ ì„ íƒëœ expertsì˜ ì§‘í•©ì´ë¼ë©´, ìµœì¢… outputì€ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ë©ë‹ˆë‹¤.

$$ y = \sum_{i \in T} p_i(x) E_i(x) $$

- E_i(x): ì„ íƒëœ expertsì—ì„œ ië²ˆì§¸ expertì˜ output
- ê·¸ëƒ¥ ê³±í•˜ê¸° í•˜ë©´ ë¨

ì´ê±¸ Pytorch ë¡œ êµ¬í˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. 

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MoELayer(nn.Module):
    def __init__(self, input_dim, output_dim, num_experts=4, k=2):
        super().__init__()
        self.k = k
        self.num_experts = num_experts

        # Router: logits = W_r Â· x
        self.router = nn.Linear(input_dim, num_experts)

        # Experts: each is a small MLP (or Linear here)
        self.experts = nn.ModuleList([
            nn.Linear(input_dim, output_dim) for _ in range(num_experts)
        ])

    def forward(self, x):
        """
        x: (batch_size, input_dim)
        returns: (batch_size, output_dim)
        """
        # Compute routing logits
        logits = self.router(x)  # (B, N)
        gate_probs = F.softmax(logits, dim=-1)  # (B, N)

        # Select top-k experts per example
        topk_vals, topk_idx = torch.topk(gate_probs, self.k, dim=-1)  # (B, k)

        # Initialize output
        output = torch.zeros(x.size(0), self.experts[0].out_features, device=x.device)

        for i in range(self.k):
            idx = topk_idx[:, i]  # expert index
            weight = topk_vals[:, i].unsqueeze(1)  # (B, 1)

            # For batch-wise selection
            expert_outputs = torch.stack([
                self.experts[expert_id](x[j].unsqueeze(0))
                for j, expert_id in enumerate(idx)
            ]).squeeze(1)  # (B, output_dim)

            output += weight * expert_outputs  # weighted sum

        return output
```


## The problem of MOE Routing

2017ë…„ë„ì— ë‚˜ì˜¨ Mixture of Expertsì˜ ëª¨ë¸ì€ ë³µì¡í•œ routingì•Œê³ ë¦¬ì¦˜ì´ í•„ìš”í–ˆìŠµë‹ˆë‹¤. 
íŠ¹íˆ 1ê°œ ì´ìƒì˜ experstsë¥¼ ì„ íƒí•˜ëŠ” ë°©ì‹ì„ ì·¨í–ˆëŠ”ë°, ê·¸ ì´ìœ ëŠ” non-trivial gradients ë¥¼ ê°–ê¸° ë•Œë¬¸ì´ë¼ê³  í•©ë‹ˆë‹¤. 

 - trivial gradients: gradients ê°’ì´ 0ì— ê°€ê¹Œìš´ ë§¤ìš° ì‘ì€ ìˆ˜ -> í•™ìŠµì´ ì•ˆë¨
 - non-trivial gradients: ì‹¤ì œë¡œ ì˜ë¯¸ìˆëŠ” (í•™ìŠµì´ ê°€ëŠ¥í•œ) gradients 

ë§Œì•½ softmax ì˜ ê²°ê³¼ê°’ ì¤‘ì—ì„œ í•˜ë‚˜ë§Œ ì·¨í•˜ê²Œ ëœë‹¤ë©´, argmax ë¥¼ ì‚¬ìš©í• ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

```python
idx = torch.argmax(logits)
```
ë¬¸ì œëŠ” argmaxë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ non-differentiable ì—°ì‚°ì´ê¸° ë•Œë¬¸ì—, backpropagationì´ ë¶ˆê°€ëŠ¥í•˜ê²Œ ë©ë‹ˆë‹¤.<br>
(ê·¸ëƒ¥ í° ê°’ì„ ì„ íƒí•´ì„œ indexë¥¼ ë¦¬í„´í•˜ëŠ” ê²ƒê¸° ë•Œë¬¸ì— discrete operation ì´ë©°, discontinuous functionì„ -> ë¯¸ë¶„ ì•ˆë¨)
ì¦‰ í•´ë‹¹ ê°’ì´ ì™œ ì„ íƒë˜ì—ˆëŠ”ì§€, gradient ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. 

ë”°ë¼ì„œ ì—¬ëŸ¬ê°œë¥¼ ì„ íƒí•˜ëŠ” ë°©ì‹ì´ í•„ìš”í–ˆìŠµë‹ˆë‹¤. 


## Expert Capacity

Switch Transformer ë¥¼ ë°°ìš°ê¸° ì „ì—, ë¨¼ì € Expert Capacity ê°œë…ì„ ì´í•´í•´ì•¼ í•©ë‹ˆë‹¤. 
 
 - ImbalanceëŠ” expert ê°€ ì„ íƒë˜ëŠ” ë¹ˆë„ì— ë”°ë¼ì„œ ë°œìƒí•˜ê¸°ë„ í•©ë‹ˆë‹¤. 

ì˜ˆë¥¼ ë“¤ì–´ì„œ "Mixture of Experts is AWESOME" ì´ë¼ëŠ” ë¬¸ì¥ì´ Routerë¥¼ ê±°ì³¤ì„ë•Œ, íŠ¹ì • Expert ë§Œ ì„ íƒëœë‹¤ë©´ undertrainingì´ ë°œìƒí• ìˆ˜ ìˆìŠµë‹ˆë‹¤.<br>

```text
Token      Softmax(logits)                  Selected Expert
------------------------------------------------------------
Mixture    [0.7, 0.1, 0.1, 0.1]             Expert 0
of         [0.8, 0.1, 0.0, 0.1]             Expert 0
Experts    [0.6, 0.1, 0.1, 0.2]             Expert 0
is         [0.5, 0.2, 0.1, 0.2]             Expert 0
AWESOME    [0.1, 0.1, 0.7, 0.1]             Expert 2
```

ì¦‰ ì–´ë–¤ expertê°€ ì„ íƒ ë˜ëŠëƒê°€ ì•„ë‹ˆë¼, ì–¼ë§ˆë‚˜ ë§ì´ íŠ¹ì • expertê°€ ì„ íƒë˜ëŠëƒê°€ ì¤‘ìš”í•©ë‹ˆë‹¤.<br>
í•´ë‹¹ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ, Expert Capacity ê°œë…ì´ ë„ì…ë˜ì—ˆìŠµë‹ˆë‹¤.<br>
íŠ¹ì • expertì˜ capacity (tokenì˜ ê°¯ìˆ˜)ë¥¼ ì •í•´ë†“ê³ , í•´ë‹¹ capacityë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš°ì—ëŠ” ë‹¤ë¥¸ expertë¥¼ ì„ íƒí•˜ë„ë¡ í•©ë‹ˆë‹¤.<br>
ì´ê²ƒì„ "token overflow" ë¼ê³  í•©ë‹ˆë‹¤. 


Transformer ëª¨ë¸ ì“°ëŠ”ë°, ì–´ë–»ê²Œ ì´ê²Œ ê°€ëŠ¥í•˜ëƒ ë¼ê³  ìƒê°ì´ ë“¤ë©´ ì•„í‚¤í…ì³ë¥¼ ë³´ë©´ ì´í•´ê°€ ë©ë‹ˆë‹¤.

```text
Input Sentence (ë¬¸ì¥)
    â†“
Tokenization
    â†“
Embedding
    â†“
Multi-Head Attention (ê³µí†µ ì²˜ë¦¬)
    â†“
FFN (tokenë§ˆë‹¤ expert ì„ íƒ!)
    â†“
Output
```

ì¦‰ FFN ë¶€ë¶„ì—ì„œ expertê°€ ì„ íƒë˜ê³  FFNê°€ ì²˜ë¦¬ë˜ê¸° ë•Œë¬¸ì—, tokenë§ˆë‹¤ ë‹¤ë¥¸ expertê°€ ì„ íƒë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.<br>
ê·¸ë˜ì„œ expert capacityë¥¼ ì •í•´ë†“ê³ , í•´ë‹¹ capacityë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš°ì—ëŠ” ë‹¤ë¥¸ expertë¥¼ ì„ íƒí•˜ë„ë¡ ì •í• ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë˜ëŠ” dropped token ì´ë¼ê³  í•´ì„œ, í•´ë‹¹ tokenì„ ì²˜ë¦¬í•˜ì§€ ì•Šê³ , ë„˜ì–´ê°€ë„ë¡ í• ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. 

$$ \text{Expert Capacity} = \left( \frac{\text{tokens_per_batch}}{\text{num_experts}} \times \text{capacity_factor} \right) $$

 - tokens_per_batch: ë°°ì¹˜ë‹¹ í† í°ì˜ ê°¯ìˆ˜
 - num_experts: ì „ì²´ expertì˜ ê°¯ìˆ˜
 - capacity_factor: expertì˜ capacityë¥¼ ì¡°ì •í•˜ëŠ” factor (ì˜ˆë¥¼ ë“¤ì–´ì„œ 1.0 ì´ë©´, ê° expertê°€ ë°°ì¹˜ë‹¹ í† í°ì˜ ê°¯ìˆ˜ / ì „ì²´ expertì˜ ê°¯ìˆ˜ ë§Œí¼ì˜ capacityë¥¼ ê°–ê²Œ ë¨)

ì˜ˆì œ 

$$ \text{Expert Capacity} = \left( \frac{100}{4} \times 1.0 \right) = 25 $$

ê° ExpertëŠ” ë°°ì¹˜ë‹¹ 25ê°œì˜ í† í°ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. <br>
ë§Œì•½ ì–´ë–¤ expertê°€ 25ê°œë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš°ì—ëŠ” ë‹¤ë¥¸ expertë¥¼ ì„ íƒí•˜ê±°ë‚˜, dropped token ìœ¼ë¡œ ê·¸ëƒ¥ ìŠ¤í‚µí•©ë‹ˆë‹¤.<br>
ë³´í†µ capacity_factorëŠ” 1.0 ì—ì„œ 1.5 ì‚¬ì´ë¡œ ì„¤ì •í•©ë‹ˆë‹¤. 

 - capacity_factor ê°€ ë†’ì„ìˆ˜ë¡: dropped tokenì´ ì¤„ì–´ë“¬ -> ë°˜ë©´ì— ì—°ì‚°ëŸ‰ì´ ëŠ˜ì–´ë‚¨
 - capacity_factor ê°€ ë‚®ì„ìˆ˜ë¡: dropped tokenì´ ëŠ˜ì–´ë‚¨ -> ë°˜ë©´ì— ì—°ì‚°ëŸ‰ì´ ì¤„ì–´ë“¬


ì•„ë˜ëŠ” Switch Transformer ì‚¬ìš©ì‹œ capacity factorê°€ ì¤„ì–´ë“¤ë©´ì„œ , ì—°ì‚°ëŸ‰ì´ ì¤„ì–´ë“œëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

<img src="{{ page.asset_path }}capacity-factor-switch-transformer.png" class="img-responsive img-rounded img-fluid center" style="border: 2px solid #333333">



# Switch Transformer


## Switch Routing: Rethinking Mixture of Experts

Switch Transformer ëŠ” ì´ëŸ¬í•œ ë³µì¡í•œ Routing ì•Œê³ ë¦¬ì¦˜ì„ ë‹¨ìˆœí™” í•˜ë©´ì„œ, ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.<br>
ì¦‰ k=1 routingì€ Switch Routingì´ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤. 

ì´ë¡œì¸ ì¸í•œ ì¥ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. 

1. Router computation ì´ ë‹¨ìˆœí•´ì§€ë©°, ì˜¤ì§ single expertë§Œ ì„ íƒí•©ë‹ˆë‹¤.
2. expert capacityê°€ ë°˜ìœ¼ë¡œ ì¤„ì–´ë“¤ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
3. routing implementaionì´ ë‹¨ìˆœí•´ì§‘ë‹ˆë‹¤. 


1ë²ˆì€ single expertë§Œ ì„ íƒí•˜ê¸° ë•Œë¬¸ì—, routing computationì´ ë‹¨ìˆœí•´ì§‘ë‹ˆë‹¤.<br>
2ë²ˆì€ expert capacityì—ì„œ ì‰½ê²Œ ë§í•˜ë©´, dropped tokenì´ ë°œìƒí•˜ë©´ì„œ ì—°ì‚°ëŸ‰ì´ ì¤„ì–´ë“¤ìˆ˜ ìˆìŠµë‹ˆë‹¤.<br>
3ë²ˆì€ ê·¸ëƒ¥ êµ¬í˜„ì´ ë‹¨ìˆœí•´ì§‘ë‹ˆë‹¤. 


## Differentiable Load Balancing Loss

$$
\begin{align}
P_i &= \frac{1}{T} \sum_{x \in \beta} p_i(x) \\
fi &= \frac{1}{T} \sum_{x \in \beta} \mathbf{1} \{ argmax p(x) = i \} \\
loss &= \alpha \cdot N \cdot \sum^N_{i=1} f_i \cdot P_i \\
\end{align} $$

- P_i
  - $$ P_i (X) $$: token xê°€ expert ië¡œ ê°ˆ í™•ë¥ . (softmaxì˜ ê²°ê³¼ê°’)
- f_i 
  - T: batchì•ˆì˜ token ê°¯ìˆ˜
  - argmax p(x): token xë¥¼ ë¼ìš°íŒ…í• ë•Œ ì„ íƒëœ expertì˜ index
  - ì‰½ê²Œ ì„¤ëª…í•˜ë©´, batch ì•ˆì˜ tokenì´ expert ië¡œ ë¼ìš°íŒ… ëëŠ”ì§€ ë¹„ìœ¨
  - ì˜ˆë¥¼ ë“¤ì–´ì„œ batchì•ˆì— tokenì´ 100ê°œê°€ ìˆê³ , ê·¸ì¤‘ì— 20ê°œê°€ expert ië¡œ ë¼ìš°íŒ… ëë‹¤ë©´, f_i = 0.2 ê°€ ë©ë‹ˆë‹¤.
- loss
  - a (alpha): scaling factor (hyperparameter ì´ë©° ë³´í†µ 0.01ë¡œ ì„¤ì •)
  - N: number of experts
  - f_i: ì‹¤ì œ routingëœ token ì¤‘ì—ì„œ expert iì— í• ë‹¹ëœ ë¹„ìœ¨ (fraction of tokens dispatched to expert i)
  - P_i: expert iì— í• ë‹¹ëœ í™•ë¥  ì´í•© (í‰ê· )

 
ì¦‰, ì‹¤ì œ token ë¶„ë°° ë¶„í¬ f_i ì™€ router (softmax)ì˜ ë¶„í¬ P_i ì˜ dot productë¥¼ ê³„ì‚° -> scaling ê³„ìˆ˜ë¥¼ ê³±í•œê²ƒ<br>
f_i (token ë¶„ë°° ë¶„í¬) ì™€ P_i (softmaxì˜ ë¶„í¬)ê°€ ì¼ì¹˜í• ìˆ˜ë¡ lossê°€ ì‘ì•„ì§‘ë‹ˆë‹¤.<br>
(ì´ë•Œ gradientê³„ì‚° í• ë•Œ f ëŠ” non-differentiable ì´ë©°, PëŠ” differentiaible ì…ë‹ˆë‹¤.)

ì¦‰ ë‹¤ìŒì€ "ì™„ë²½íˆ ê· í˜• ì¡íŒ ìƒíƒœ (ideal case)" ì…ë‹ˆë‹¤. 

```text
Expert:       E1     E2     E3     E4
f_i:          0.25   0.25   0.25   0.25   (ì‹¤ì œ ë¼ìš°íŒ… ë¶„í¬)
P_i:          0.25   0.25   0.25   0.25   (softmax ê¸°ëŒ€ í™•ë¥ )

f_i * P_i:    0.0625 0.0625 0.0625 0.0625
Sum(f â‹… P):   0.25   (ìµœì†Œê°’!)
```

ë§Œì•½ í¸í–¥ëœ ìƒíƒœê°€ ëœë‹¤ë©´, ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

```text
Expert:       E1     E2     E3     E4
f_i:          0.70   0.10   0.10   0.10   (ê±°ì˜ E1ë§Œ ì“°ì„)
P_i:          0.40   0.20   0.20   0.20   (softmaxë„ ì•½ê°„ E1 ì¹˜ìš°ì¹¨)

f_i * P_i:    0.28   0.02   0.02   0.02
Sum(f â‹… P):   0.34   (ì»¤ì§ â†’ loss â†‘)
```


Router Implementation

```python
import torch
import torch.nn.functional as F
from torch import nn

class Router(nn.Module):
    """
    The router module determines which expert each token is sent to.
    It also computes the load balancing loss.
    """
    def __init__(self, d_model, num_experts):
        super().__init__()
        self.num_experts = num_experts
        self.gate = nn.Linear(d_model, num_experts)

    def forward(self, x):
        # x: (batch_size, seq_len, d_model)
        logits = self.gate(x)
        # logits: (batch_size, seq_len, num_experts)
        
        # Get the top-1 expert for each token
        top1_logits, top1_indices = logits.max(dim=-1)
        # top1_indices: (batch_size, seq_len)
        
        # Create a one-hot encoding of the expert indices
        # This will be used to dispatch tokens to the correct expert
        expert_mask = F.one_hot(top1_indices, self.num_experts).float()
        # expert_mask: (batch_size, seq_len, num_experts)
        
        # Calculate the load balancing loss
        # This loss encourages all experts to be used equally.
        
        # Count how many tokens are sent to each expert
        tokens_per_expert = expert_mask.sum(dim=(0, 1)) # (num_experts)
        # Total number of tokens
        total_tokens = x.size(0) * x.size(1)
        
        # Calculate the fraction of tokens sent to each expert
        fraction_tokens_per_expert = tokens_per_expert / total_tokens
        
        # Calculate the expert probabilities from the logits
        expert_probs = F.softmax(logits, dim=-1).mean(dim=(0, 1)) # (num_experts)
        
        # The load balancing loss is the dot product of these two quantities
        load_balancing_loss = self.num_experts * torch.dot(fraction_tokens_per_expert, expert_probs)
        
        return expert_mask, load_balancing_loss
```

## Expert Capacity 


# Implementation 

ì—¬ê¸°ì„œ ì „ì²´ ì½”ë“œê°€ ì•„ë‹Œ í•µì‹¬ì´ ë˜ëŠ” ì½”ë“œë§Œ ê³µìœ  í•©ë‹ˆë‹¤. 


## Switch Transformer Model

```text
Input Tokens
     â”‚
     â–¼
[Embedding Layer] â”€â”€â–º nn.Embedding(ntoken, d_model)
     â”‚
     â–¼
[Positional Encoding] â”€â”€â–º PositionalEncoding(d_model, dropout)
     â”‚
     â–¼
[Transformer Encoder Stack]
     â”‚
     â””â”€â–º (L layers of)
           â””â”€â–º [Multi-head Attention]
           â””â”€â–º [LayerNorm + Residual]
           â””â”€â–º [Switch FFN (MoE Layer)]
               â””â”€â–º Router â†’ Expert selection
               â””â”€â–º Dispatch token to Expert
               â””â”€â–º Gather output
           â””â”€â–º [LayerNorm + Residual]
     â”‚
     â–¼
[Final Linear Decoder] â”€â”€â–º nn.Linear(d_model, ntoken)
     â”‚
     â–¼
Vocabulary Logits (for language modeling)
```


```python
class SwitchTransformerLM(nn.Module):
    def __init__(self, ntoken: int, d_model: int, nhead: int, d_ff: int, num_experts: int, num_layers: int,
                 dropout: float = 0.5):
        super().__init__()
        self.model_type = 'Transformer'
        self.d_model = d_model
        self.encoder = nn.Embedding(ntoken, d_model)
        self.pos_encoder = PositionalEncoding(d_model, dropout)

        encoder_layer = SwitchTransformerEncoderLayer(d_model, nhead, d_ff, num_experts, dropout)
        self.transformer_encoder = SwitchTransformerEncoder(encoder_layer, num_layers)

        self.decoder = nn.Linear(d_model, ntoken)
```

## SwitchTransformer Encoder Layer  + MoE Layer

```text
Input: Token Hidden States (src)
     â”‚
     â–¼
[Multi-Head Self-Attention]
     â”‚
     â–¼
ğŸŸ¦ MoELayer  â† â† â† â† â† â† â† â† â† â† â† â† â† ğŸ§  í•µì‹¬ í¬ì¸íŠ¸!
     â”‚
     â”œâ”€â–º Router(x)
     â”‚     â””â”€â–º Routing logits (W_r Â· x)
     â”‚     â””â”€â–º Softmax â†’ Top-1 Expert ì„ íƒ
     â”‚     â””â”€â–º expert_mask ìƒì„± (one-hot)  
     â”‚
     â”œâ”€â–º Capacity ê³„ì‚° (token overflow ë°©ì§€)
     â”‚
     â”œâ”€â–º ê° Expert ë³„ë¡œ:
     â”‚     â””â”€ Token dispatch (x[expert_indices])
     â”‚     â””â”€ Expert FFN ì²˜ë¦¬
     â”‚     â””â”€ Output scatter to original positions
     â”‚
     â””â”€â–º ìµœì¢… Output (sparse computation ê²°ê³¼)
             + Load Balancing Loss
     â–¼
Dropout + Residual
     â–¼
LayerNorm #2
     â–¼
Output Hidden States, Load Balancing Loss
```

SwitchTransformerEncoderLayer ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„ì€ ì•„ë˜ ì½”ë“œì´ë©°,<br>
ê¸°ì¡´ TransformerEncoderLayerì™€ MoELayerë¥¼ ê²°í•©í•œ ê²ƒì…ë‹ˆë‹¤.<br>
Attention ì—°ì‚° í›„ì— MoE layerë¥¼ ì ìš©í•˜ì—¬, ê° í† í°ë§ˆë‹¤ ë‹¤ë¥¸ expertë¥¼ ì„ íƒí•˜ê³  ì²˜ë¦¬í•©ë‹ˆë‹¤.<br>

```python
class SwitchTransformerEncoderLayer(nn.Module):
    def __init__(self, d_model: int, nhead: int, d_ff: int, num_experts: int, dropout: float = 0.1) -> None:
        super().__init__()
        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        self.moe_layer = MoELayer(d_model, d_ff, num_experts)

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, src: Tensor,
                src_mask: Optional[Tensor] = None,
                src_key_padding_mask: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:
        # Multi-head self-attention
        src2 = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]
        src = src + self.dropout(src2)
        src = self.norm1(src)

        # MoE layer
        src2, load_balancing_loss = self.moe_layer(src)
        src = src + self.dropout(src2)
        src = self.norm2(src)

        return src, load_balancing_loss
```


## MoE Layer 

```text
Input (x): (batch_size, seq_len, d_model)
   â”‚
   â–¼
Router (W_r Â· x â†’ softmax â†’ top-1 expert ì„ íƒ)
   â”‚
   â–¼
Expert Mask (one-hot): (batch_size, seq_len, num_experts)
   â”‚
   â–¼
Flatten: x â†’ (BÃ—T, d_model), mask â†’ (BÃ—T, num_experts)
   â”‚
   â–¼
for each expert i in num_experts:
    - token ì„ íƒ (í•´ë‹¹ expertë¡œ ë¼ìš°íŒ…ëœ ê²ƒë§Œ)
    - capacity ì´ˆê³¼ ì‹œ overflow drop
    - expert_i(input) â†’ FFN ì²˜ë¦¬
    - ê²°ê³¼ë¥¼ final_output ì— ë‹¤ì‹œ index_add

   â–¼
Reshape: (BÃ—T, d_model) â†’ (batch_size, seq_len, d_model)

Return: final_output, load_balancing_loss
```



```python

class MoELayer(nn.Module):
    """
    The Mixture-of-Experts (MoE) layer, which replaces the FFN layer in a standard Transformer.
    """
    def __init__(self, d_model: int, d_ff: int, num_experts: int, capacity_factor: float = 1.25) -> None:
        super().__init__()
        self.d_model = d_model
        self.num_experts = num_experts
        self.capacity_factor = capacity_factor

        self.router = Router(d_model, num_experts)
        self.experts = nn.ModuleList([Expert(d_model, d_ff) for _ in range(num_experts)])
        
    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor]:
        """
        Example: Let's say we have:
        - batch_size=2, seq_len=4, d_model=8, num_experts=2
        - Input x: shape (2, 4, 8) - 2 sequences, each with 4 tokens of 8 dimensions
        """
        # x: (batch_size, seq_len, d_model)
        # Example: x.shape = (2, 4, 8)
        batch_size, seq_len, d_model = x.shape
        
        # Get the expert mask and load balancing loss from the router
        # The router decides which expert should process each token
        expert_mask, load_balancing_loss = self.router(x)
        # expert_mask: (batch_size, seq_len, num_experts)
        # Example: expert_mask.shape = (2, 4, 2)
        #   expert_mask[0, 0, :] = [1, 0] means token 0 goes to expert 0
        #   expert_mask[0, 1, :] = [0, 1] means token 1 goes to expert 1
        
        # Determine the capacity of each expert
        capacity = int((seq_len / self.num_experts) * self.capacity_factor)
        
        # Reshape tensors for easier processing - flatten batch and sequence dimensions
        x_flat = x.view(-1, d_model) # (batch_size * seq_len, d_model)
        expert_mask_flat = expert_mask.view(-1, self.num_experts) # (batch_size * seq_len, num_experts)

        # Initialize output tensor with zeros - same shape as flattened input
        final_output = torch.zeros_like(x_flat)
        # Example: final_output.shape = (8, 8)
        
        # Process each expert separately
        for i, expert in enumerate(self.experts):
            # Find which tokens should go to this expert (non-zero entries in the mask)
            expert_indices = torch.where(expert_mask_flat[:, i] > 0)[0]
            # Example for expert 0: expert_indices might be [0, 2, 4] (tokens 0, 2, 4)
            # Example for expert 1: expert_indices might be [1, 3, 5, 6, 7] (tokens 1, 3, 5, 6, 7)
            
            # Apply capacity constraint - if too many tokens assigned, keep only the first 'capacity' tokens
            if expert_indices.shape[0] > capacity:
                expert_indices = expert_indices[:capacity]
                # Example: if expert 1 has 5 tokens but capacity=2, keep only [1, 3]

            # Process tokens assigned to this expert
            if expert_indices.shape[0] > 0:
                # Extract the tokens that should go to this expert
                expert_input = x_flat[expert_indices]
                # Example: if expert_indices=[1, 3], expert_input.shape = (2, 8)
                
                # Process the tokens through the expert network
                expert_output = expert(expert_input)
                # Example: expert_output.shape = (2, 8) - same as expert_input
                
                # Ensure dtype consistency
                expert_output = expert_output.to(final_output.dtype)

                # Place the expert's output back to the final output at the correct positions
                # Switch Transformer uses exclusive routing: each token goes to exactly ONE expert
                # So we use assignment (=) not addition (+=)
                # Example: if expert_indices=[1, 3] and expert_output has 2 rows
                #   final_output[1] = expert_output[0]  # Place expert's output for token 1
                #   final_output[3] = expert_output[1]  # Place expert's output for token 3
                for j, token_idx in enumerate(expert_indices):
                    final_output[token_idx] = expert_output[j]

        # Reshape back to original dimensions
        final_output = final_output.view(batch_size, seq_len, d_model)
        # Example: final_output.shape = (2, 4, 8) - back to original shape
        
        return final_output, load_balancing_loss
```