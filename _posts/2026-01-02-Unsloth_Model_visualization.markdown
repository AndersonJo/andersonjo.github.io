---
layout: post
title: "Unsloth - Model Visualization"
date: 2026-01-02 01:00:00
categories: "unsloth"
asset_path: /assets/images/
tags: []
---


# Model Visualization

```python
from transformers import BatchEncoding, TextStreamer
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/gpt-oss-20b",
    dtype=None,  # torch.bfloat16,  # None for auto detection
    max_seq_length=1000,
    load_in_4bit=False,
    full_finetuning=False,
    low_cpu_mem_usage=True,
    device_map="cuda",  # Explicitly load to CUDA
)
```

VIsualization Codes

```python
import torch
from rich.tree import Tree
from rich import print as rprint

def visualize_model_structure(model):
    # 1. Create root node
    tree = Tree(f"ğŸ—ï¸ [bold blue]Model: {getattr(model.config, '_name_or_path', 'Unknown')}[/bold blue]")
    
    # Dictionary to keep track of created nodes: {path_string: rich_tree_node}
    node_lookup = {"": tree}

    for name, module in model.named_modules():
        if name == "": continue
        
        # Split path: 'model.layers.0.self_attn' -> ['model', 'layers', '0', 'self_attn']
        parts = name.split('.')
        parent_path = ".".join(parts[:-1])
        current_part = parts[-1]

        # Calculate Size Info
        # Get parameter count for this specific module
        params_count = sum(p.numel() for p in module.parameters(recurse=False))
        
        # Get shape if it's a leaf layer (like Linear or Embedding)
        shape_info = ""
        if hasattr(module, 'weight') and isinstance(module.weight, torch.Tensor):
            shape_info = f" [yellow]({list(module.weight.shape)})[/yellow]"
        elif params_count > 0:
            shape_info = f" [green]({params_count:,} params)[/green]"

        # 2. Find or Create Node
        if parent_path in node_lookup:
            parent_node = node_lookup[parent_path]
            # Add new node with style and size info
            new_node = parent_node.add(f"[bold magenta]{current_part}[/bold magenta]{shape_info}")
            node_lookup[name] = new_node

    rprint(tree)

# Execution
visualize_model_structure(model)
```

here's the result

```bash
ğŸ—ï¸ Model: unsloth/gpt-oss-20b
â”œâ”€â”€ model
â”‚   â”œâ”€â”€ embed_tokens ([201088, 2880])
â”‚   â”œâ”€â”€ layers
â”‚   â”‚   â”œâ”€â”€ 0
â”‚   â”‚   â”‚   â”œâ”€â”€ self_attn (64 params)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ q_proj ([4096, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ k_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ v_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ o_proj ([2880, 4096])
â”‚   â”‚   â”‚   â”œâ”€â”€ mlp
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ router ([32, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ experts (796,538,880 params)
â”‚   â”‚   â”‚   â”œâ”€â”€ input_layernorm ([2880])
â”‚   â”‚   â”‚   â””â”€â”€ post_attention_layernorm ([2880])
â”‚   â”‚   â”œâ”€â”€ 1
â”‚   â”‚   â”‚   â”œâ”€â”€ self_attn (64 params)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ q_proj ([4096, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ k_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ v_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ o_proj ([2880, 4096])
â”‚   â”‚   â”‚   â”œâ”€â”€ mlp
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ router ([32, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ experts (796,538,880 params)
â”‚   â”‚   â”‚   â”œâ”€â”€ input_layernorm ([2880])
â”‚   â”‚   â”‚   â””â”€â”€ post_attention_layernorm ([2880])
â”‚   â”‚   â”œâ”€â”€ 2
â”‚   â”‚   â”‚   â”œâ”€â”€ self_attn (64 params)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ q_proj ([4096, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ k_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ v_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ o_proj ([2880, 4096])
â”‚   â”‚   â”‚   â”œâ”€â”€ mlp
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ router ([32, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ experts (796,538,880 params)
â”‚   â”‚   â”‚   â”œâ”€â”€ input_layernorm ([2880])
â”‚   â”‚   â”‚   â””â”€â”€ post_attention_layernorm ([2880])
â”‚   â”‚   â”œâ”€â”€ 3
â”‚   â”‚   â”‚   â”œâ”€â”€ self_attn (64 params)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ q_proj ([4096, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ k_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ v_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ o_proj ([2880, 4096])
â”‚   â”‚   â”‚   â”œâ”€â”€ mlp
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ router ([32, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ experts (796,538,880 params)
â”‚   â”‚   â”‚   â”œâ”€â”€ input_layernorm ([2880])
â”‚   â”‚   â”‚   â””â”€â”€ post_attention_layernorm ([2880])
â”‚   â”‚   â”œâ”€â”€ 4
â”‚   â”‚   â”‚   â”œâ”€â”€ self_attn (64 params)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ q_proj ([4096, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ k_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ v_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ o_proj ([2880, 4096])
â”‚   â”‚   â”‚   â”œâ”€â”€ mlp
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ router ([32, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ experts (796,538,880 params)
â”‚   â”‚   â”‚   â”œâ”€â”€ input_layernorm ([2880])
â”‚   â”‚   â”‚   â””â”€â”€ post_attention_layernorm ([2880])
â”‚   â”‚   â”œâ”€â”€ 5
â”‚   â”‚   â”‚   â”œâ”€â”€ self_attn (64 params)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ q_proj ([4096, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ k_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ v_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ o_proj ([2880, 4096])
â”‚   â”‚   â”‚   â”œâ”€â”€ mlp
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ router ([32, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ experts (796,538,880 params)
â”‚   â”‚   â”‚   â”œâ”€â”€ input_layernorm ([2880])
â”‚   â”‚   â”‚   â””â”€â”€ post_attention_layernorm ([2880])
â”‚   â”‚   â”œâ”€â”€ 6
â”‚   â”‚   â”‚   â”œâ”€â”€ self_attn (64 params)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ q_proj ([4096, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ k_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ v_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ o_proj ([2880, 4096])
â”‚   â”‚   â”‚   â”œâ”€â”€ mlp
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ router ([32, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ experts (796,538,880 params)
â”‚   â”‚   â”‚   â”œâ”€â”€ input_layernorm ([2880])
â”‚   â”‚   â”‚   â””â”€â”€ post_attention_layernorm ([2880])
â”‚   â”‚   â”œâ”€â”€ 7
â”‚   â”‚   â”‚   â”œâ”€â”€ self_attn (64 params)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ q_proj ([4096, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ k_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ v_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ o_proj ([2880, 4096])
â”‚   â”‚   â”‚   â”œâ”€â”€ mlp
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ router ([32, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ experts (796,538,880 params)
â”‚   â”‚   â”‚   â”œâ”€â”€ input_layernorm ([2880])
â”‚   â”‚   â”‚   â””â”€â”€ post_attention_layernorm ([2880])
â”‚   â”‚   â”œâ”€â”€ 8
â”‚   â”‚   â”‚   â”œâ”€â”€ self_attn (64 params)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ q_proj ([4096, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ k_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ v_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ o_proj ([2880, 4096])
â”‚   â”‚   â”‚   â”œâ”€â”€ mlp
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ router ([32, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ experts (796,538,880 params)
â”‚   â”‚   â”‚   â”œâ”€â”€ input_layernorm ([2880])
â”‚   â”‚   â”‚   â””â”€â”€ post_attention_layernorm ([2880])
â”‚   â”‚   â”œâ”€â”€ 9
â”‚   â”‚   â”‚   â”œâ”€â”€ self_attn (64 params)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ q_proj ([4096, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ k_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ v_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ o_proj ([2880, 4096])
â”‚   â”‚   â”‚   â”œâ”€â”€ mlp
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ router ([32, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ experts (796,538,880 params)
â”‚   â”‚   â”‚   â”œâ”€â”€ input_layernorm ([2880])
â”‚   â”‚   â”‚   â””â”€â”€ post_attention_layernorm ([2880])
â”‚   â”‚   â”œâ”€â”€ 10
â”‚   â”‚   â”‚   â”œâ”€â”€ self_attn (64 params)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ q_proj ([4096, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ k_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ v_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ o_proj ([2880, 4096])
â”‚   â”‚   â”‚   â”œâ”€â”€ mlp
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ router ([32, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ experts (796,538,880 params)
â”‚   â”‚   â”‚   â”œâ”€â”€ input_layernorm ([2880])
â”‚   â”‚   â”‚   â””â”€â”€ post_attention_layernorm ([2880])
â”‚   â”‚   â”œâ”€â”€ 11
â”‚   â”‚   â”‚   â”œâ”€â”€ self_attn (64 params)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ q_proj ([4096, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ k_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ v_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ o_proj ([2880, 4096])
â”‚   â”‚   â”‚   â”œâ”€â”€ mlp
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ router ([32, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ experts (796,538,880 params)
â”‚   â”‚   â”‚   â”œâ”€â”€ input_layernorm ([2880])
â”‚   â”‚   â”‚   â””â”€â”€ post_attention_layernorm ([2880])
â”‚   â”‚   â”œâ”€â”€ 12
â”‚   â”‚   â”‚   â”œâ”€â”€ self_attn (64 params)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ q_proj ([4096, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ k_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ v_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ o_proj ([2880, 4096])
â”‚   â”‚   â”‚   â”œâ”€â”€ mlp
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ router ([32, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ experts (796,538,880 params)
â”‚   â”‚   â”‚   â”œâ”€â”€ input_layernorm ([2880])
â”‚   â”‚   â”‚   â””â”€â”€ post_attention_layernorm ([2880])
â”‚   â”‚   â”œâ”€â”€ 13
â”‚   â”‚   â”‚   â”œâ”€â”€ self_attn (64 params)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ q_proj ([4096, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ k_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ v_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ o_proj ([2880, 4096])
â”‚   â”‚   â”‚   â”œâ”€â”€ mlp
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ router ([32, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ experts (796,538,880 params)
â”‚   â”‚   â”‚   â”œâ”€â”€ input_layernorm ([2880])
â”‚   â”‚   â”‚   â””â”€â”€ post_attention_layernorm ([2880])
â”‚   â”‚   â”œâ”€â”€ 14
â”‚   â”‚   â”‚   â”œâ”€â”€ self_attn (64 params)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ q_proj ([4096, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ k_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ v_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ o_proj ([2880, 4096])
â”‚   â”‚   â”‚   â”œâ”€â”€ mlp
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ router ([32, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ experts (796,538,880 params)
â”‚   â”‚   â”‚   â”œâ”€â”€ input_layernorm ([2880])
â”‚   â”‚   â”‚   â””â”€â”€ post_attention_layernorm ([2880])
â”‚   â”‚   â”œâ”€â”€ 15
â”‚   â”‚   â”‚   â”œâ”€â”€ self_attn (64 params)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ q_proj ([4096, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ k_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ v_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ o_proj ([2880, 4096])
â”‚   â”‚   â”‚   â”œâ”€â”€ mlp
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ router ([32, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ experts (796,538,880 params)
â”‚   â”‚   â”‚   â”œâ”€â”€ input_layernorm ([2880])
â”‚   â”‚   â”‚   â””â”€â”€ post_attention_layernorm ([2880])
â”‚   â”‚   â”œâ”€â”€ 16
â”‚   â”‚   â”‚   â”œâ”€â”€ self_attn (64 params)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ q_proj ([4096, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ k_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ v_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ o_proj ([2880, 4096])
â”‚   â”‚   â”‚   â”œâ”€â”€ mlp
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ router ([32, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ experts (796,538,880 params)
â”‚   â”‚   â”‚   â”œâ”€â”€ input_layernorm ([2880])
â”‚   â”‚   â”‚   â””â”€â”€ post_attention_layernorm ([2880])
â”‚   â”‚   â”œâ”€â”€ 17
â”‚   â”‚   â”‚   â”œâ”€â”€ self_attn (64 params)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ q_proj ([4096, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ k_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ v_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ o_proj ([2880, 4096])
â”‚   â”‚   â”‚   â”œâ”€â”€ mlp
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ router ([32, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ experts (796,538,880 params)
â”‚   â”‚   â”‚   â”œâ”€â”€ input_layernorm ([2880])
â”‚   â”‚   â”‚   â””â”€â”€ post_attention_layernorm ([2880])
â”‚   â”‚   â”œâ”€â”€ 18
â”‚   â”‚   â”‚   â”œâ”€â”€ self_attn (64 params)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ q_proj ([4096, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ k_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ v_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ o_proj ([2880, 4096])
â”‚   â”‚   â”‚   â”œâ”€â”€ mlp
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ router ([32, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ experts (796,538,880 params)
â”‚   â”‚   â”‚   â”œâ”€â”€ input_layernorm ([2880])
â”‚   â”‚   â”‚   â””â”€â”€ post_attention_layernorm ([2880])
â”‚   â”‚   â”œâ”€â”€ 19
â”‚   â”‚   â”‚   â”œâ”€â”€ self_attn (64 params)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ q_proj ([4096, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ k_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ v_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ o_proj ([2880, 4096])
â”‚   â”‚   â”‚   â”œâ”€â”€ mlp
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ router ([32, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ experts (796,538,880 params)
â”‚   â”‚   â”‚   â”œâ”€â”€ input_layernorm ([2880])
â”‚   â”‚   â”‚   â””â”€â”€ post_attention_layernorm ([2880])
â”‚   â”‚   â”œâ”€â”€ 20
â”‚   â”‚   â”‚   â”œâ”€â”€ self_attn (64 params)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ q_proj ([4096, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ k_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ v_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ o_proj ([2880, 4096])
â”‚   â”‚   â”‚   â”œâ”€â”€ mlp
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ router ([32, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ experts (796,538,880 params)
â”‚   â”‚   â”‚   â”œâ”€â”€ input_layernorm ([2880])
â”‚   â”‚   â”‚   â””â”€â”€ post_attention_layernorm ([2880])
â”‚   â”‚   â”œâ”€â”€ 21
â”‚   â”‚   â”‚   â”œâ”€â”€ self_attn (64 params)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ q_proj ([4096, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ k_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ v_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ o_proj ([2880, 4096])
â”‚   â”‚   â”‚   â”œâ”€â”€ mlp
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ router ([32, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ experts (796,538,880 params)
â”‚   â”‚   â”‚   â”œâ”€â”€ input_layernorm ([2880])
â”‚   â”‚   â”‚   â””â”€â”€ post_attention_layernorm ([2880])
â”‚   â”‚   â”œâ”€â”€ 22
â”‚   â”‚   â”‚   â”œâ”€â”€ self_attn (64 params)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ q_proj ([4096, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ k_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ v_proj ([512, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ o_proj ([2880, 4096])
â”‚   â”‚   â”‚   â”œâ”€â”€ mlp
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ router ([32, 2880])
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ experts (796,538,880 params)
â”‚   â”‚   â”‚   â”œâ”€â”€ input_layernorm ([2880])
â”‚   â”‚   â”‚   â””â”€â”€ post_attention_layernorm ([2880])
â”‚   â”‚   â””â”€â”€ 23
â”‚   â”‚       â”œâ”€â”€ self_attn (64 params)
â”‚   â”‚       â”‚   â”œâ”€â”€ q_proj ([4096, 2880])
â”‚   â”‚       â”‚   â”œâ”€â”€ k_proj ([512, 2880])
â”‚   â”‚       â”‚   â”œâ”€â”€ v_proj ([512, 2880])
â”‚   â”‚       â”‚   â””â”€â”€ o_proj ([2880, 4096])
â”‚   â”‚       â”œâ”€â”€ mlp
â”‚   â”‚       â”‚   â”œâ”€â”€ router ([32, 2880])
â”‚   â”‚       â”‚   â””â”€â”€ experts (796,538,880 params)
â”‚   â”‚       â”œâ”€â”€ input_layernorm ([2880])
â”‚   â”‚       â””â”€â”€ post_attention_layernorm ([2880])
â”‚   â”œâ”€â”€ norm ([2880])
â”‚   â””â”€â”€ rotary_emb
â””â”€â”€ lm_head ([201088, 2880])
```